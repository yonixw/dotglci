
## TODO

cli validate options deno
    https://cliffy.io/docs@v0.25.7/command
        https://www.npmjs.com/package/command-line-usage
        https://www.npmjs.com/package/command-line-args
        https://www.npmjs.com/package/commander#typescript
        https://examples.deno.land/command-line-arguments

convert bash scripts to deno
    https://www.npmjs.com/package/docker-compose
    https://www.npmjs.com/package/dockerode ??

bring zod, so we can validate yaml? and export json schema? are they supported in deno?
    yaml start to get heavy!
    export schema task
    ?? do it with json-schema after manual export ts to json schema?

See if deno pckg https://www.npmjs.com/package/@gitbeaker/rest
    has runners new way

seperate "load var" to basic,project so we can use it also for manual vars (get from path, from .env etc)

gitlab features:
    custom ci path gitlap update
    secure files add/remove?

run manual pipeline(s):
    //https://docs.gitlab.com/ee/api/pipelines.html#create-a-new-pipeline
    variables (move manual here)
    ref to use

wait for job(s)
    //https://docs.gitlab.com/ee/api/pipelines.html#get-the-latest-pipeline
    wait for latest + each of manual run
    verify all env are seen (also filter[env])

download artifacts/logs from all watched runs

windows script to run the dockers
    dind (double start slash): 
        https://tomgregory.com/running-docker-in-docker-on-windows/

wont do:
    pass all host env to runner docker
        too risky, like aws creds in env and much much more (privacy stuff also like 'host')
    custom event source such as: web interface, merge request etc... 
        it's just another variables, use manual variable to decide jobs condition like if (X=merge or mock=merge) then...
    pipeline triggers
        same as custom event source, mock it yourself, or use manual pipelines (they both can pass only ref+vars)
    terraform
        https://registry.terraform.io/providers/gitlabhq/gitlab/latest/docs
        not justified as we still need to run, watch, download through API calls
        also not looking like any pipeline resources ...
